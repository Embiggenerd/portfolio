<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    * {
      margin: 0;
      padding: 0;
    }

    .article-container {
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: flex-start;
    }

    p {
      max-width: 600px;
    }

    .canvas-demo {
      border: black solid 1px;
    }
  </style>
  <title>How node actually works, part 2</title>
</head>

<body>
  <div class="article-container">

    <h1>How node actually works, part 2: the event loop</h1>
    <p>
      How much could one more inch hurt? We're talking about having confidence in 
      Javascript interviews, and what do you say when you're asked "what is node", 
      or "what does asynchronous mean in node." How node talks to the operating system 
      is explained in part 1 . Now that we know we are receiving a multiplexed stream 
      of events which are file descriptors ready to be read from or listened to, we 
      are ready to talk about what we do with those events.
    </p>

    <pre>
      1. const a = () => {
      2.   console.log('hi')
      3.   return
      4. }
      5. const b = () => {
      6.   a()
      7.   console.log('planet') 
      8.   return
      9. }
     10. a()
     11. b()
     12. console.log('all done')
  </pre>

    <p>
      Our CPU has to worry about more than just the console logs, it has to worry about the order of instructions 
      that it carries out. The first line it cares about is 10, and then it will want to look up the definition of 
      a, which really starts at line two. When it returns, we then skip to line 11. How does our CPU keep track of 
      that? How does it know to go back to 11?
    </p>

    <p>
      Let's divide our system memory into two parts:
    </p>

    <img src="article-images/memoryDiagram.png"/>
    
    <p>
      Before you get confused, here's what everything means: our memory is a zero indexed structure. We load up our 
      instructions starting at index 0. These instructions are similar to our javascript code, but not exactly. Call 
      and return get their own instructions, and beside those, we only call stdout. These are the 'real' instructions 
      that any human readable language gets translated into. This structure grows up, as you would expect.
    </p>


    <p>
      Starting at index 240, we have a stack. This stack grows down, which is confusing, and means that when we push 
      onto the stack, we have to decrement the index which is the 'top' of the stack, which we call the stack pointer, 
      or SP. When we pop, we will increment it. This memory only goes to index 255, which makes it an 8 bit CPU 
      emulator. Above index 240, we have locations where we can store things we don't have to worry about today.
    </p>

    <p>
      Most importantly, the index of the currently executing instruction is stored in the program counter, or PC. A part 
      of our flow of execution is incrementing the PC to execute the next instruction, of course unless our instruction 
      specifically sets the PC to something else, which we will discuss next.
    </p>

    <p>
      The <b>call</b> instruction does two things.
    </p>

    <ul>
      <li>Decrements the SP.</li>
      <li>
        It pushes the return address onto the stack. This is a reference to the instruction we should execute after the 
        current function returns.
      </li>
      <li>
        It sets PC to the current instruction's definition. This is really a reference to the first instruction in the 
        definition.
      </li>
    </ul>

    <p>
      Here's what our memory looks like after we run call on line 10:
    </p>

    <img src="article-images/memoryDiagram_1.png"/>

    <p>
      We have the SP point to line 11 in the stack, and our PC points to line 2. We execute line two, which prints 'hi' to 
      the user, and set PC to the return on line 3. People don't ordinarily put explicit returns at the end of void 
      functions, but here we are trying to be really transparent.
    </p>

    <p>
      The <b>return</b> instruction
    </p>

    <ul>
      <li>
        Pops the return address off the stack at the SP.
      </li>
      <li>
        Stores the return address in the PC.
      </li>
      <li>
        Increments the SP.
      </li>
    </ul>

    <p>
      Our stack currently only has line 11 on it, so we pop it, and point the PC to it. At this point, or SP is pointing 
      to an empty memory[240], and or PC is at memory[239]. That instruction happens to be line 12, which is an stdout, 
      and our PC will point to line 6, which is another call. Here's how those two steps look.
    </p>

    <img src="article-images/memoryDiagram_2.png"/>

    <img src="article-images/memoryDiagram_3.png"/>

    <p>
      A nested function causes our stack to grow to size 2, and now we are starting to see the point of the stack. The 
      user will see a second 'hi', then 'planet', then 'all done', as we reach return statements in our script.
    </p>
    
    <p>
      What is the point of explaining how an 8 bit CPU emulation could handle a JS script? When we say a function is 
      blocked, we mean to say that a return statement, which pops the next function off the stack, won't get to store 
      it in the program counter in order to execute it. That's because the stack operations and the script's functions 
      are all in the same thread of execution. In order for asynchronous code to work, we have to curate the order 
      instructions are executed. The structure that sends messages to the emulator we will call a message queue, because
      each instruction is a message, and we append to the bottom and read off the top.
    </p>

    <p>
      Let's think about why blocking code is a problem for us. If we need to make a network call, and we are waiting for
      some server to respond to us, this could take a long time. Imagine writing an application where regular application
      logic has to wait perhaps a secon for an API call to return before we move onto executing the rest of the code in our
      app. It would make things unusable. 
    </p>

    <p>
      The classic solution is to use a thread to wait on the response, and when the call comes in on the other thread, we 
      interrupt the current thread, and insert our data. However, as we discussed in the 
      <a href="https://medium.com/@igor.atakhanov/acid-mvcc-and-how-postgresql-handles-concurrent-queries-27fb48692db8">
        previous article
      </a>, we avoid threads because of the memory requirements of each thread and the latency overhread of context switching. Also,
      how would we orchestrate the interruption? Our node server is constantly executing handler functions, most of which is
      synchronous code, so when do we insert our network data? We need a scheduler, and that scheduler is called the event loop.
    </p>

    <h2>
      Event loop concept.
    </h2>

    <p>
      Understanding the event loop conceptually, and the reason for it, should happen before understanding it in detail. Why
      we would use a thread for something like a network call, is to get that network call out of our javascript thread of 
      execution, and not block the rest of the script. To get away from threads, we must depend on something else. Previously 
      we talked about how the operating system uses signal interrupt to let us know when a network call comes back, and it this
      takes the form of an event in our event stream. If when we make the network call, we register it somewhere, then ignore 
      it until the data comes back, we can basically just do our synchronous stuff the register is told that the response came
      in, then we can do something with that data. Sound familiar? It's pretty similar to our select system call - we cam poll
      the registered asych events, and when one of them gets back with data, we can read the data, and do something with it.
    </p>

    <p>
      With select, we would loop through our registered file descriptors, and send back just the ones ready for a read or write
      operation, and this could be a thousand different files. However, with things like network events, we can place all network
      i/o events into one bucket - the network io bucket, and we can check that bucket, as well as other types of buckets. For 
      instance, using setTimeout(callback, timeout) registers a function in the setTimeout bucket, and at the beginnign of every loop, we can 
      check the current time, and if it is greater than the timeout, we can invoke the callback. Then we can check the other bucket,
      until we loop back around. That is the idea behind the event loop. 
    </p>

    <p>
      Steps in our loop
    </p>
    
    <ol>
      <li>Check what time it is</li>
      <li>Check if timers set in setTimeout should invoke the callback</li>
      <li>Check if timer in setInterval should invoke the calback</li>
      <li>Check callbacks we may have postponed from last time</li>
      <li>Check for I/O events and their callbacks</li>
      <li>Check if there are any setImmediate callbacks to run</li>
      <li>Check for any close callbacks, such as connection.on('close'...</li>
    </ol>

    <p>
      As you can see, there's a specific order, and some of it may leave you scratching your head. Why is setImmediate not the
      first step? Because I/O and timer callbacks often use setImmediate to make sure they delay certain code from happening, 
      and we want to get to that code as soon as possible.
    </p>

    <p>
      Another question is, how does this talk to our little emulator? How do we get these callbacks to run, if we have synchronous
      code blocking on our call stack? Well, sometimes the call stack is empty, and that's when we check if there are any async
      callbacks to execute. You may assume we place these instructions on their own queue, and you're right, and we can call this
      queue the callback queue for now. 
    </p>

    <p>
      This is what our model looks like now.
    </p>

    <img src="article-images/eventLoopDiagram.png"/>

    <p>
      So far, we have sort of figured out how to place async code onto our sync call stack to execute it. We have a loop
      checking for async events to get done, then we place the callback associated with those events onto our sync callstack
      only when it is empty. This would explain this sort of code:
    </p>

    <pre>
      console.log('Script Start')
      setTimeout(()=>{
        console.log('First SetTimeout')
        setTimeout(()=>console.log('Second SetTimeout'), 0)
      }, 0)
      setTimeout(()=>console.log('Third SetTimeout'))
      console.log('Script End')

      // Output:
      // Script Start
      // Script End
      // First SetTimeout
      // Third SetTimeout
      // Second SetTimeout
    </pre>

    <p>
      Notice, our Script End message is printed, then we start with the setTimeouts. This is because until we pop the last
      sync instruction off the call stack, it is not empty, and the callback queue is not even checked. The first
    </p>

    <p>
      So, we have these large tasks, but what about things like thenables? Remember writing
    </p>
    
  </div>
</body>

</html>